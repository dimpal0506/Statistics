{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Statistics Advance - 1\n",
        "#Assignment Questions\n",
        "\n",
        "#1. Explain the properties of the F-distribution.\n",
        "\n",
        "#The F-distribution is a probability distribution that arises in various statistical tests, particularly in hypothesis testing.\n",
        "#It is characterized by the following properties:\n",
        "#1. Shape:\n",
        "#The F-distribution is asymmetric and skewed to the right.\n",
        "#The shape of the distribution depends on two parameters: the degrees of freedom for the numerator (df1) and the degrees of freedom for the denominator (df2).\n",
        "\n",
        "#2. Range:\n",
        "#The F-distribution is defined for all non-negative values.\n",
        "#The values of the F-statistic can range from 0 to infinity.\n",
        "\n",
        "#3. Degrees of Freedom:\n",
        "#The F-distribution has two degrees of freedom:\n",
        "#df1: Degrees of freedom for the numerator.\n",
        "#df2: Degrees of freedom for the denominator.\n",
        "#These degrees of freedom determine the specific shape of the distribution.\n",
        "\n",
        "#4. Mean and Variance:\n",
        "#The mean of the F-distribution is:\n",
        "#Mean = df2 / (df2 - 2) for df2 > 2\n",
        "#The variance of the F-distribution is:\n",
        "#Variance = (2 * df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)) for df2 > 4\n",
        "\n",
        "#5. Relationship with Other Distributions:\n",
        "#The F-distribution is related to the chi-squared distribution.\n",
        "#It can be derived from the ratio of two independent chi-squared random variables, each divided by their respective degrees of freedom.\n",
        "\n",
        "#6. Applications:\n",
        "#The F-distribution is used in various statistical tests, including:\n",
        "#ANOVA (Analysis of Variance): To compare the means of multiple groups.\n",
        "#Testing the equality of two variances: To determine if two populations have the same variance.\n",
        "#Regression analysis: To assess the overall significance of a regression model.\n",
        "#The F-distribution is a crucial tool in statistical analysis, allowing us to make inferences about population parameters based on sample data."
      ],
      "metadata": {
        "id": "fAOFcxarZmIV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "#The F-distribution is primarily used in two main types of statistical tests:\n",
        "\n",
        "#1. Analysis of Variance (ANOVA):\n",
        "#Purpose: ANOVA is used to compare the means of two or more groups to determine if there are significant differences between them.\n",
        "#Why F-distribution is appropriate:\n",
        "#In ANOVA, the F-statistic is calculated as the ratio of the variance between groups to the variance within groups.\n",
        "#Under the null hypothesis (i.e., all group means are equal), this ratio follows an F-distribution.\n",
        "#A significant F-statistic indicates that there is evidence to reject the null hypothesis and conclude that at least one group mean is different from the others.\n",
        "\n",
        "#2. Testing the Equality of Two Variances:\n",
        "#Purpose: This test is used to determine if the variances of two populations are equal.\n",
        "#Why F-distribution is appropriate:\n",
        "#The F-statistic is calculated as the ratio of the two sample variances.\n",
        "#Under the null hypothesis (i.e., the two population variances are equal), this ratio follows an F-distribution.\n",
        "#A significant F-statistic suggests that the two population variances are likely different.\n",
        "\n",
        "#Key Points:\n",
        "#In both cases, the F-distribution provides a framework for evaluating the significance of the observed differences or ratios.\n",
        "#The degrees of freedom for the numerator and denominator in the F-distribution are determined by the specific test and the sample sizes involved.\n",
        "#The F-distribution is particularly useful in situations where the underlying assumptions of normality and equal variances are met.\n",
        "#By understanding the properties of the F-distribution and its applications in ANOVA and variance comparison tests, you can effectively analyze data and draw meaningful conclusions.\n",
        "\n"
      ],
      "metadata": {
        "id": "otsey6Y8ZmE1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "#To conduct an F-test to compare the variances of two populations, the following key assumptions must be met:\n",
        "#Normality: Both populations from which the samples are drawn should be normally distributed.\n",
        "#While the F-test is relatively robust to deviations from normality, especially with larger sample sizes, significant departures can affect the accuracy of the test.\n",
        "\n",
        "#Independence: The samples drawn from the two populations must be independent of each other.\n",
        "#This means that the selection of one sample should not influence the selection of the other.\n",
        "\n",
        "#Equal Variances (Homoscedasticity): This assumption, while it might seem counterintuitive for a test comparing variances, is actually crucial.\n",
        "#The F-test assumes that the two populations have equal variances. If this assumption is violated,\n",
        "#the F-test may not be valid, and alternative tests like the Welch's t-test or the Brown-Forsythe test might be more appropriate.\n",
        "#It's important to note that while these assumptions are crucial,\n",
        "#the F-test is relatively robust to violations of the normality assumption, especially with larger sample sizes. However, the assumption of independence and equal variances should be carefully considered and potentially tested before conducting the F-test."
      ],
      "metadata": {
        "id": "J9NXZNITZmBS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "#Purpose of ANOVA\n",
        "#Analysis of Variance (ANOVA) is a statistical technique used to compare the means of two or more groups.\n",
        "#It helps us determine whether there are significant differences between the means of these groups.\n",
        "\n",
        "#Difference between ANOVA and t-test:\n",
        "#While both ANOVA and t-tests are used to compare means, their key difference lies in the number of groups being compared:\n",
        "\n",
        "#t-test: Used to compare the means of two groups.\n",
        "#ANOVA: Used to compare the means of more than two groups.\n",
        "\n",
        "#Why ANOVA is preferred over multiple t-tests:\n",
        "#Reduced Type I Error Rate: Conducting multiple t-tests increases the likelihood of making a Type I error (falsely rejecting the null hypothesis).\n",
        "#ANOVA controls this risk by performing a single overall test.\n",
        "#Efficiency: ANOVA is more efficient as it analyzes all groups simultaneously, reducing the number of comparisons needed.\n",
        "\n",
        "#In essence:\n",
        "#t-test: A one-on-one comparison.\n",
        "#ANOVA: A group-wide comparison.\n",
        "#By using ANOVA, we can efficiently determine if there are significant differences among multiple groups without inflating the Type I error rate.\n"
      ],
      "metadata": {
        "id": "X0PfOMvAbQOi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "#When to Use One-Way ANOVA Instead of Multiple t-tests\n",
        "#You should use a one-way ANOVA instead of multiple t-tests when comparing the means of more than two groups under the following conditions:\n",
        "\n",
        "#1. Controlling Type I Error Rate:\n",
        "#Multiple Comparisons Problem: Conducting multiple t-tests increases the likelihood of making a Type I error (falsely rejecting the null hypothesis).\n",
        "#This is because each test has a certain probability of incorrectly rejecting the null hypothesis.\n",
        "#ANOVA's Advantage: ANOVA performs a single overall test, controlling the Type I error rate at a specified level (e.g., 0.05).\n",
        "#This helps to maintain the overall significance level.\n",
        "\n",
        "#2. Increased Statistical Power:\n",
        "#Pooling Variability: ANOVA pools the variability within each group, providing a more precise estimate of the overall variability.\n",
        "#This can lead to increased statistical power, making it easier to detect significant differences between groups.\n",
        "\n",
        "#3. Efficiency:\n",
        "#Simpler Analysis: ANOVA requires fewer calculations and is less computationally intensive than conducting multiple t-tests,\n",
        "#especially as the number of groups increases.\n",
        "\n",
        "#In summary, one-way ANOVA is a more efficient and statistically rigorous approach for comparing the means of multiple groups,\n",
        "#especially when controlling the Type I error rate is a concern.\n",
        "#However, it's important to note that if the ANOVA reveals a significant difference between groups,\n",
        "#you may need to conduct post-hoc tests (such as Tukey's HSD or Bonferroni) to determine which specific groups differ from each other.\n",
        "#These post-hoc tests can be thought of as pairwise comparisons, but they are adjusted to control the overall Type I error rate."
      ],
      "metadata": {
        "id": "6NFuljkYZl9t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.How does this partitioning contribute to the calculation of the F-statistic?\n",
        "#Partitioning Variance in ANOVA\n",
        "#In ANOVA, the total variance in a dataset is partitioned into two components:\n",
        "#Between-Group Variance: This represents the variation in the means of different groups. It measures how much the group means deviate from the overall mean.\n",
        "#Within-Group Variance: This represents the variation within each group. It measures how much the individual data points deviate from their respective group means.\n",
        "\n",
        "#Calculating the F-Statistic\n",
        "#The F-statistic is calculated as the ratio of the between-group variance to the within-group variance:\n",
        "#F = (Between-Group Variance) / (Within-Group Variance)\n",
        "#Numerator (Between-Group Variance): This reflects the variability between the group means. If the group means are significantly different, the between-group variance will be larger.\n",
        "#Denominator (Within-Group Variance): This reflects the random error or variability within each group.\n",
        "\n",
        "#Interpreting the F-Statistic\n",
        "#A larger F-statistic indicates that the between-group variance is significantly larger than the within-group variance.\n",
        "#This suggests that the differences between the group means are unlikely to be due to chance.\n",
        "#By comparing the calculated F-statistic to a critical value from the F-distribution,\n",
        "#we can determine whether to reject the null hypothesis (that all group means are equal) and conclude that there are significant differences between the groups.\n",
        "\n",
        "#In essence, ANOVA partitions the total variance to assess the relative contributions of group differences and random error.\n",
        "#The F-statistic quantifies this comparison, allowing us to draw inferences about the underlying population means."
      ],
      "metadata": {
        "id": "G8Y_K6i3c-Qs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach.\n",
        "#What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "#Classical (Frequentist) vs. Bayesian ANOVA\n",
        "#While both classical and Bayesian approaches aim to analyze variance and compare means,\n",
        "#they differ significantly in their philosophical underpinnings and methodological approaches.\n",
        "\n",
        "#Classical (Frequentist) ANOVA\n",
        "#Uncertainty: Treats parameters as fixed but unknown quantities. Uncertainty is quantified in terms of confidence intervals and p-values.\n",
        "#Parameter Estimation: Uses point estimates (e.g., sample means and variances) to estimate population parameters.\n",
        "#Hypothesis Testing: Formulates null and alternative hypotheses, calculates test statistics (like the F-statistic),\n",
        "#and determines significance levels to make decisions.\n",
        "\n",
        "#Bayesian ANOVA\n",
        "#Uncertainty: Treats parameters as random variables with probability distributions. Uncertainty is quantified in terms of probability distributions,\n",
        "#often represented by credible intervals.\n",
        "#Parameter Estimation: Uses Bayes' theorem to update prior beliefs about parameters based on observed data.\n",
        "#This results in posterior distributions that represent the uncertainty in the parameter estimates.\n",
        "#Hypothesis Testing: Directly calculates the probability of the data under different hypotheses, allowing for more nuanced interpretations of the results.\n",
        "#Bayesian hypothesis testing often involves calculating Bayes factors to compare the evidence for different models.\n",
        "\n",
        "#Key Differences:\n",
        "#Philosophical Underpinnings: Classical statistics is based on frequentist probability, where probabilities are interpreted as long-run frequencies.\n",
        "#Bayesian statistics is based on Bayesian probability, where probabilities represent degrees of belief.\n",
        "#Focus: Classical ANOVA focuses on hypothesis testing and p-values. Bayesian ANOVA focuses on estimating parameters and their uncertainty.\n",
        "#Flexibility: Bayesian ANOVA is more flexible, allowing for the incorporation of prior information and more complex models.\n",
        "\n",
        "#In essence:\n",
        "#Classical ANOVA is more focused on making decisions based on fixed significance levels and point estimates.\n",
        "#Bayesian ANOVA provides a more flexible and probabilistic framework for understanding uncertainty and making inferences.\n",
        "#When to Use Which Approach\n",
        "\n",
        "#Classical ANOVA: Suitable for traditional hypothesis testing and making definitive decisions.\n",
        "#Bayesian ANOVA: Useful when prior information is available, when the goal is to quantify uncertainty, or when more nuanced interpretations of the data are desired.\n",
        "#In conclusion, while both approaches can be used to analyze variance and compare means, the Bayesian approach offers a more comprehensive and\n",
        "#flexible framework for understanding uncertainty and making inferences."
      ],
      "metadata": {
        "id": "PuqpZDKRc-NA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Question: You have two sets of data representing the incomes of two different professions\n",
        "#Profession A: [48, 52, 55, 60, 62'\n",
        "#Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "#Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "#bjective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for Profession A and B\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate the F-statistic and p-value\n",
        "f_statistic, p_value = stats.f_oneway(profession_a, profession_b)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "\n",
        "#If the p-value is less than the significance level (usually 0.05), we reject the null hypothesis and\n",
        "#conclude that the variances of the two professions' incomes are significantly different.\n",
        "#If the p-value is greater than the significance level, we fail to reject the null hypothesis and\n",
        "#conclude that there is not enough evidence to suggest that the variances are different."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxAeSvCc-KK",
        "outputId": "cc7258fa-6bd3-4b13-e745-271975dba1ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 3.232989690721649\n",
            "p-value: 0.10987970118946545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\n",
        "#Region A: [160, 162, 165, 158, 164]\n",
        "#Region B: [172, 175, 170, 168, 174]\n",
        "#Region C: [180, 182, 179, 185, 183]\n",
        "#Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "#Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpretation:\n",
        "if p_value < 0.05:\n",
        "    print(\"There is a significant difference in average heights between the three regions.\")\n",
        "else:\n",
        "    print(\"There is no significant difference in average heights between the three regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVxlwlMBc-HT",
        "outputId": "51342b3c-d958-4245-e4b2-831ea6c651fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "There is a significant difference in average heights between the three regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2dvHfYflc-Ea"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}